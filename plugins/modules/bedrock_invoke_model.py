#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright: Contributors to the Ansible project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)


DOCUMENTATION = r"""
---
module: bedrock_invoke_model
short_description: Runs inference using Amazon Bedrock models
version_added: "1.0.0"
description:
    - This module invokes a specified model on Amazon Bedrock with a user-provided request body.
    - The request body is passed as raw bytes or JSON (dict), depending on the model requirements.
    - The module supports customizing request and response MIME types.
author:
    - Alina Buzachis (@alinabuzachis)
options:
    model_id:
        description:
            - The ID of the Bedrock model to invoke.
        type: str
        required: true
    body:
        description:
            - The request body to send to the model.
            - Must be a JSON object, string, or plain text depending on the model.
        type: raw
        required: true
    content_type:
        description:
            - Desired MIME type of the response.
        type: str
        default: "application/json"
    accept:
        description:
            - The desired MIME type of the inference response.
        type: str
        default: "application/json"
    trace:
        description:
            - Specifies whether to enable or disable Bedrock tracing.
        type: str
        choices: ['ENABLED', 'DISABLED', 'ENABLED_FULL']
        default: 'DISABLED'
    guardrail_identifier:
        description:
            - The unique identifier of the guardrail to use.
            - This option requires boto3 >= 1.35.8.
        type: str
    guardrail_version:
        description:
            - The version number for the guardrail. Can also be 'DRAFT'.
            - O(guardrail_version) is required when O(guardrail_identifier) is specified.
            - This option requires boto3 >= 1.35.8.
        type: str
    performance_config_latency:
        description:
            - Model performance settings for the request, optimizing for latency.
            - This option requires boto3 >= 1.35.8.
        type: str
        choices: ['standard', 'optimized']
        default: 'standard'
extends_documentation_fragment:
    - amazon.aws.common.modules
    - amazon.aws.region.modules
    - amazon.aws.boto3
"""


EXAMPLES = r"""
- name: Invoke Claude v3 Haiku with custom JSON body
  amazon.ai.bedrock_invoke_model:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    body:
      messages:
        - role: "user"
          content:
            - type: "text"
              text: "What is the key to a good cup of coffee?"
      max_tokens: 250
    content_type: "application/json"
    accept: "application/json"

- name: Invoke a model with plain text input
  amazon.ai.bedrock_invoke_model:
    model_id: "some-text-model"
    body: "Once upon a time..."
    content_type: "text/plain"
    accept: "text/plain"
"""


RETURN = r"""
completion:
    description: The text (if extractable) generated by the Bedrock model in response.
    type: str
    returned: success
    sample: >
        "It looks like you're interested in testing a prompt! Here are a few examples of different
        types of prompts you might find useful, depending on what you're aiming to achieve:\n\n
        ### Creative Writing Prompt\n\"Write a short story about a world"
model_id:
    description: The ID of the Bedrock model that was invoked.
    type: str
    returned: success
    sample: "anthropic.claude-3-haiku-20240307-v1:0"
raw_response:
    description: >
        The full, unprocessed JSON response returned by Bedrock. Useful for debugging
        or accessing provider-specific metadata.
    type: dict
    returned: always
    sample: {
        "output": {
            "message": {
                "content": [
                    {
                        "text": "It looks like you're interested in testing a prompt! Here are a few
                                 examples of different types of prompts you might find useful,
                                 depending on what you're aiming to achieve:\n\n### Creative Writing
                                 Prompt\n\"Write a short story about a world"
                    }
                ],
                "role": "assistant"
            }
        },
        "stopReason": "max_tokens",
        "usage": {
            "cacheReadInputTokenCount": 0,
            "cacheWriteInputTokenCount": 0,
            "inputTokens": 3,
            "outputTokens": 50,
            "totalTokens": 53
        }
    }
"""


import json
from typing import Any
from typing import Dict
from typing import Union

from ansible.module_utils.common.dict_transformations import camel_dict_to_snake_dict

try:
    import botocore
except ImportError:
    pass  # Handled by AnsibleAWSModule

from ansible_collections.amazon.aws.plugins.module_utils.exceptions import AnsibleAWSError
from ansible_collections.amazon.aws.plugins.module_utils.modules import AnsibleAWSModule
from ansible_collections.amazon.aws.plugins.module_utils.retries import AWSRetry


def encode_body(body: Union[dict, list, str]) -> bytes:
    """
    Convert the body into bytes for invoke_model.

    Args:
        body: Input request body (dict, list, or string).

    Returns:
        Bytes representation of the body.
    """
    if isinstance(body, (dict, list)):
        return json.dumps(body).encode("utf-8")
    elif isinstance(body, str):
        return body.encode("utf-8")
    else:
        raise TypeError("body must be a dict, list, or string")


def extract_completion(response_body: Union[Dict[str, Any], str]) -> Union[str, None]:
    """
    Extract a human-readable answer from the Bedrock model response.

    Supports:
      - Anthropic: response_body["completion"]
      - Nova / Amazon: response_body["output"]["message"]["content"][0]["text"]
      - Cohere-style: response_body["generations"][0]["text"]

    Args:
        response_body: Parsed response (dict) or raw string.

    Returns:
        Normalized text string or None if extraction fails.
    """
    if isinstance(response_body, dict):
        if "completion" in response_body:  # Anthropic
            return response_body["completion"]
        if "output" in response_body:  # OpenAI-style
            try:
                return response_body["output"]["message"]["content"][0]["text"]
            except (KeyError, IndexError, TypeError):
                return None
        if "generations" in response_body:  # Cohere
            try:
                return response_body["generations"][0]["text"]
            except (KeyError, IndexError, TypeError):
                return None
    return str(response_body) if isinstance(response_body, str) else None


def build_optional_params(module: AnsibleAWSModule) -> Dict[str, Any]:
    """
    Build optional API parameters from module inputs.

    Args:
        module: The AnsibleAWSModule instance.

    Returns:
        Dictionary of optional parameters.
    """
    params: Dict[str, Any] = {}
    if module.params["trace"] != "DISABLED":
        params["trace"] = module.params["trace"]

    if module.params.get("guardrail_identifier") and module.params.get("guardrail_version"):
        if not module.require_boto3_at_least("1.35.8"):
            module.warn(
                "guardrail_identifier and guardrail_version require botocore >= 1.35.8. guardrail_identifier and guardrail_version will be ignored."
            )
        else:
            params["guardrailIdentifier"] = module.params["guardrail_identifier"]
            params["guardrailVersion"] = module.params["guardrail_version"]
    if module.params["performance_config_latency"] != "standard":
        if not module.require_boto3_at_least("1.35.8"):
            module.warn(
                "performance_config_latency and guardrail_version require botocore >= 1.35.8. performance_config_latency and guardrail_version will be ignored."
            )
        else:
            params["performanceConfigLatency"] = module.params["performance_config_latency"]
    return params


def main():
    module_args = dict(
        model_id=dict(type="str", required=True),
        body=dict(type="raw", required=True),
        content_type=dict(type="str", default="application/json"),
        accept=dict(type="str", default="application/json"),
        trace=dict(type="str", choices=["ENABLED", "DISABLED", "ENABLED_FULL"], default="DISABLED"),
        guardrail_identifier=dict(type="str", required=False),
        guardrail_version=dict(type="str", required=False),
        performance_config_latency=dict(type="str", choices=["standard", "optimized"], default="standard"),
    )

    module = AnsibleAWSModule(
        argument_spec=module_args,
        supports_check_mode=True,
        required_together=[["guardrail_identifier", "guardrail_version"]],
    )

    if module.check_mode:
        module.exit_json(changed=True)

    model_id: str = module.params["model_id"]
    body_bytes: bytes = encode_body(module.params["body"])
    content_type: str = module.params["content_type"]
    accept: str = module.params["accept"]

    optional_params = build_optional_params(module)

    try:
        client = module.client("bedrock-runtime", retry_decorator=AWSRetry.jittered_backoff())
    except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e:
        module.fail_json_aws(e, msg="Failed to connect to AWS.")

    try:
        response = client.invoke_model(
            body=body_bytes,
            modelId=model_id,
            contentType=content_type,
            accept=accept,
            **optional_params,
        )

        raw_output = response["body"].read().decode("utf-8")

        try:
            response_body = camel_dict_to_snake_dict(json.loads(raw_output))
        except Exception:
            response_body = raw_output

        completion = extract_completion(response_body)

        module.exit_json(
            changed=True,
            model_id=model_id,
            completion=completion,
            raw_response=response_body,
        )

    except AnsibleAWSError as e:
        module.fail_json_aws_error(e)


if __name__ == "__main__":
    main()
