#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright: Contributors to the Ansible project
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)


DOCUMENTATION = r"""
---
module: bedrock_inference
short_description: Runs inference using Amazon Bedrock models
version_added: "1.0.0"
description:
    - This module invokes a specified model on Amazon Bedrock with a given prompt to run inference.
author:
    - Alina Buzachis (@alinabuzachis)
options:
    model_id:
        description:
            - The ID of the Bedrock model to invoke.
        type: str
        required: true
    prompt:
        description:
            - The text prompt to send to the model.
        type: str
        required: true
    trace:
        description:
            - Specifies whether to enable or disable Bedrock tracing.
        type: str
        choices: ['ENABLED', 'DISABLED', 'ENABLED_FULL']
        default: 'DISABLED'
    guardrail_identifier:
        description:
            - The unique identifier of the guardrail to use.
        type: str
    guardrail_version:
        description:
            - The version number for the guardrail. Can also be 'DRAFT'.
        type: str
    performance_config_latency:
        description:
            - Model performance settings for the request, optimizing for latency.
        type: str
        choices: ['standard', 'optimized']
        default: 'standard'
    max_tokens:
        description:
            - The maximum number of tokens to generate in the response.
        type: int
        default: 250
extends_documentation_fragment:
    - amazon.aws.common.modules
    - amazon.aws.region.modules
    - amazon.aws.boto3
"""


EXAMPLES = r"""
- name: Invoke Claude v3 Haiku with a guardrail
  amazon.ai.bedrock_inference:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    prompt: "What is the key to a good cup of coffee?"
    guardrai_identifier: "example-guardrail-id"
    guardrail_version: "1"
  register: bedrock_response

- name: Invoke a model with tracing enabled
  amazon.ai.bedrock_inference:
    model_id: "anthropic.claude-3-haiku-20240307-v1:0"
    prompt: "Write a haiku about a mountain stream."
    trace: "ENABLED_FULL"
  register: bedrock_response_with_trace
"""


RETURN = r"""
completion:
    description: The text generated by the Bedrock model in response to the prompt.
    type: str
    returned: success
    sample: "The key to a good cup of coffee is starting with freshly roasted beans and grinding them just before brewing."
model_id:
    description: The ID of the Bedrock model that was invoked.
    type: str
    returned: success
    sample: "anthropic.claude-3-haiku-20240307-v1:0"
prompt:
    description: The original prompt that was sent to the model.
    type: str
    returned: success
    sample: "What is the key to a good cup of coffee?"
changed:
    description: Indicates whether the task made changes to the system. This module will always return true on success.
    type: bool
    returned: always
    sample: true
"""


import json

try:
    import botocore
except ImportError:
    pass  # Handled by AnsibleAWSModule

from ansible_collections.amazon.aws.plugins.module_utils.exceptions import AnsibleAWSError
from ansible_collections.amazon.aws.plugins.module_utils.modules import AnsibleAWSModule
from ansible_collections.amazon.aws.plugins.module_utils.retries import AWSRetry


def main():
    module_args = dict(
        model_id=dict(type="str", required=True),
        prompt=dict(type="str", required=True),
        trace=dict(type="str", choices=["ENABLED", "DISABLED", "ENABLED_FULL"], default="DISABLED"),
        guardrail_identifier=dict(type="str", required=False),
        guardrail_version=dict(type="str", required=False),
        performance_config_latency=dict(type="str", choices=["standard", "optimized"], default="standard"),
        max_tokens=dict(type="int", default=250, no_log=True),
    )

    module = AnsibleAWSModule(
        argument_spec=module_args,
        supports_check_mode=True,
    )

    if module.check_mode:
        module.exit_json(changed=False)

    model_id = module.params["model_id"]
    prompt = module.params["prompt"]
    max_tokens = module.params["max_tokens"]

    # Initialize a dictionary to hold optional parameters for the API call
    optional_params = {}
    if module.params["trace"] != "DISABLED":
        optional_params["trace"] = module.params["trace"]
    if module.params.get("guardrail_identifier"):
        optional_params["guardrailIdentifier"] = module.params["guardrail_identifier"]
        # The guardrailVersion is required if guardrailIdentifier is provided
        if not module.params.get("guardrail_version"):
            module.fail_json(msg="guardrail_version is required when guardrail_identifier is specified.")
        optional_params["guardrailVersion"] = module.params["guardrail_version"]
    if module.params["performance_config_latency"] != "standard":
        optional_params["performanceConfigLatency"] = module.params["performance_config_latency"]

    try:
        client = module.client("bedrock-runtime", retry_decorator=AWSRetry.jittered_backoff())
    except (botocore.exceptions.ClientError, botocore.exceptions.BotoCoreError) as e:
        module.fail_json_aws(e, msg="Failed to connect to AWS.")

    body = {"prompt": f"\n\nHuman: {prompt}\n\nAssistant:", "max_tokens_to_sample": max_tokens}

    try:
        response = client.invoke_model(
            body=json.dumps(body), modelId=model_id, contentType="application/json", accept="*/*", **optional_params
        )

        response_body = json.loads(response["body"].read())
        completion = response_body.get("completion")

        module.exit_json(changed=True, model_id=model_id, prompt=prompt, completion=completion)

    except AnsibleAWSError as e:
        module.fail_json_aws_error(e)


if __name__ == "__main__":
    main()
